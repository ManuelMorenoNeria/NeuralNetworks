As we have seen before, a neural network consists of the following elements:

- Inputs
- Weights
- Biases
- Weighted Sum
- Activation Function

![51549981077_3a7a520c81_b](https://github.com/ManuelMorenoNeria/NeuralNetworks/assets/114908218/da85bd68-1a55-49ce-ab79-272db89f3b5e)

These elements operate as follows:

**1. Multiply the input parameters by the weights of the first layer**

**2. Add the bias to the product of the multiplication**

**3. Apply the activation function of the next layer to the previous result. Before continuing, let's explain the purpose of applying an activation function.**

Activation functions allow a neural network to **learn more complex and flexible relationships** between input data and output. They are **not limited to simple linear relationships** (like a straight line). Instead of trying to fit the data to a strict, direct relationship as a straight line would, activation functions enable the network to learn and model nonlinear relationships, such as curves or more complex shapes.

**5. Send the result to the next layer**

**6. Repeat the process**

